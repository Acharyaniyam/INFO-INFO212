{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3b338f5d-6b2f-44ca-bdaf-60d535b0ee50","showTitle":false,"title":""},"id":"JsSPg12FcT6m"},"source":["# Drexel University\n","## College of Computing and Informatics\n","## INFO 323: Cloud Computing and Big Data\n","### Assignment 3: Spark Data Analysis\n","### Due Date: Sunday, May 18, 2025\n","This assignment counts for 15% of the final grade\n","\n","\n","### A. Assignment Overview\n","This assignment provides the opportunity for you to practice with Spark programming basics.\n","\n","### B. What to Hand In\n","\n","Sumbit a completed this Jupyter notebook.\n","\n","### C. How to Hand In\n","\n","Submit your Jupyter notebook file through the course website in the Blackboard Learn system.\n","\n","### D. When to Hand In\n","\n","1. Submit your assignment no later than 11:59pm in the due date.\n","2. There will be a 10% (absolute value) deduction for each day of lateness, to a maximum of 3 days; assignments will not be accepted beyond that point. Missing work will earn a zero grade.\n","\n","### E. Answer the following questions\n","### Note: All programming should be done in Spark, except in cases where you are specifically instructed to use Pandas."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ffc26a5a-6931-4495-9c3d-cb81e8a19b45","showTitle":false,"title":""},"id":"9yQa4EorcT6p"},"source":["# YOUR NAME:"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3382c272-59a6-44a9-8ef3-7f0d37efc032","showTitle":false,"title":""},"tags":[],"id":"dv5Z5zNrcT6p"},"outputs":[],"source":["# import packages\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"896d3ac7-a049-47f5-b967-6a266f6ca7c3","showTitle":false,"title":""},"tags":[],"id":"6BNPzLxJcT6q"},"outputs":[],"source":["# check availability of Spark\n","spark.version"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"69581143-27f9-448f-8462-227af1956d9a","showTitle":false,"title":""},"id":"R0xNgkO8cT6r"},"source":["### Download Data and ingest To GCP Bucket:\n","1. Create a subfolder `2023_flights` under your GCP storage bucket. Download and ingest the flights data in year 2023 to the `2023_flgiths` subfolder.\n","2. Create a subfolder `2024_flights` under your GCP storage bucket. Download and ingest the flights data in year 2024 to the `2024_flgiths` subfolder.\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1425a5f9-33fb-4a3f-973f-d5b82a73710e","showTitle":false,"title":""},"id":"ATBDCpiYcT6r"},"source":["### Bayesian Analysis:\n","1. In this assignment, you will explore the 2023 flights data to create a Bayes model.\n","2. You will use the 2024 flights data to evaluate the Bayes model in terms of accuracy, precision, and recall."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"51f31726-b1ec-4cf6-81a7-95fa4522890b","showTitle":false,"title":""},"id":"LdjZxOpxcT6r"},"source":["## 1. Create a Spark DataFrame `flights` from the 12 CSV files in the 2023_flights subfolder in your GCP Bucket. (If you use Databricks Community Edition which allows up to 10GB data, ingest the CSV files to Databricks and create a Spark DataFrame from the ingested data.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rLy9QVfcT6r"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"dc5a7e3e-0fb7-42dd-8438-9f34e1785675","showTitle":false,"title":""},"id":"UQ_ffJtEcT6s"},"source":["## 2. Print the Schema of the Spark DataFrame `flights`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28aICzs1cT6s"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fbb996e7-98a8-4a18-af8f-4561d4e0647a","showTitle":false,"title":""},"id":"UQqMc_XpcT6s"},"source":["## 3. Create a relational view `flights_table` for Spark SQL queries. Show the number of rows in the view."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BY9a_DqncT6s"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5-tK0WKucT6s"},"source":["# In the following, you can use either the Spark DataFrame `flights` or the relational view `flights_table`."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5bb14fd7-11ad-4088-baca-987e4e940239","showTitle":false,"title":""},"id":"aXPc-hHUcT6s"},"source":["## 4. List all the flights (Origin, Dest, Distance, FlightDate, Reporting_Airline) from PHL (origin) ordered by the distances in descending order."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wmh3jIxUcT6s"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"dcf3ea5b-81b0-4363-97c7-88117fbdf398","showTitle":false,"title":""},"id":"4XYZOpd7cT6t"},"source":["## 5.  Print the summary statistics for the column: ArrDelay, DepDelay and Distance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvL7CQnacT6t"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3f8913ea-6df9-4a9d-9903-b60e627e4dd7","showTitle":false,"title":""},"id":"4JeoEMlBcT6t"},"source":["## 6. Drop rows with missing values in the column: ArrDelay, DepDelay, and Distance. Save the results in a Spark DataFrame `flights_dropna`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbGCXP-PcT6t"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0a042c58-ba72-4207-bec8-4b1fd06b1fc8","showTitle":false,"title":""},"id":"6RLkjMZTcT6t"},"source":["## 7. How many rows are dropped at the previous step?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msyLMnUXcT6t"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9d1e33ac-b0c6-4624-a8bd-d33f5499d56a","showTitle":false,"title":""},"id":"-0cRJ8ltcT6t"},"source":["## 8. Compute correlation between the two columns: ArrDelay and DepDelay in `flights_dropna`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNv4vP-8cT6t"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b2558031-3a2d-4cbf-a847-b7d20a21249a","showTitle":false,"title":""},"id":"1bLNJGrHcT6t"},"source":["## 9. Extract a Spark DataFrame `df_spark` from `flights_dropna` with three columns 'Distance',  'DepDelay', and 'ArrDelay' such that the Distance is less than 2000 miles and the DepDelay is between -10 and 40 minutes. Show the summary statistics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czQw9gvvcT6t"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"758e02ff-5829-4e16-ae97-67ebacbbad66","showTitle":false,"title":""},"id":"pZDP7xqKcT6t"},"source":["## 10. Add a new column 'ontime' to the Spark DataFrame `df_spark` with the following values:\n","- 'ontime' = true if the ArrDelay is less than or equal to 15 minutes.\n","- Otherwise, 'ontime' = false\n","## Name the new Spark DataFrame as `df_spark_ontime`. Show the first 5 rows of `df_spark_ontime`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUt9HnSkcT6t"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"babd88d1-6600-4369-99ff-963695030f06","showTitle":false,"title":""},"id":"iZRPLzHIcT6t"},"source":["## 11. We want to visualize the joint distribution between Distance and DepDelay. Since the size of data is large, we will sample 10% of the data from the Spark DataFrame `df_spark_ontime` and convert the samples to a Pandas DataFrame named `df_pd`. Show the summary statistics of `df_pd`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORhgzvJtcT6t"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b9db71f9-0731-45b6-8c33-c11a8c329777","showTitle":false,"title":""},"id":"HhuSXOgMcT6t"},"source":["## 12. Use Seaborn jointplot() to make a hexbin plot between the distributions of 'Distance' and 'Depdelay' of the Pandas DataFrame `df_pd`. Each hexagon of the hexbin plot is colored based on the number of flights in that bin, with darker hexagons indicating more flights. Discuss your observations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MImT1YOHcT6t"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4cyIbEKLcT6t"},"source":["## Discuss the plot:"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6b3ea68f-080a-4fb3-8fc8-aae6cea654ed","showTitle":false,"title":""},"id":"gYtgivmPcT6u"},"source":["## 13. For the rest, continue to work on the Spark DataFrame `df_spark_ontime`. Use Spark approxQuantile() to create 4 bins based on 'Distance'. The bins should contain approximately equal number of flights. Store the bin boundaries in a list 'distthresh'. Show the content of list."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L5RGq2rMcT6u"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b31635fa-043e-4a8a-aa72-fcc6ccd44ab4","showTitle":false,"title":""},"id":"D5xXS36EcT6u"},"source":["## 14. Similary, use Spark approxQuantile() to create 4 bins based on 'DepDelay'. The bins should contains approximately equal number of flights. Store the bin boundaries in a list 'delaythresh'. Show the content of the list."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rmo1RdEgcT6u"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d0706bb4-54f9-47a3-88f6-3b4f303daa93","showTitle":false,"title":""},"id":"-SuGt03LcT6u"},"source":["## 15. Compute the fraction (probability) of ontime flights in each combination of the 'Distance' and 'DepDelay' bins. Store the results in a Pandas DataFrame `df_decision` with the columns ['dist_thresh', 'delay_thresh', 'frac_ontime'], where 'dist_thresh' and 'delay_thresh' represent the bin boundaries, and 'frac_ontime' is the percentage of flights that are on time (ontime = True) within each bin. Show the DataFrame which has 4x4=16 records."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14HbSrWNcT6u"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Q9VWJdBcT6u"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2767a351-6106-4e28-b7a1-95c1ffc33516","showTitle":false,"title":""},"id":"m7wGuKAycT6z"},"source":["## 16.  Sort the rows of the Pandas DataFrame `df_decision` by 'frac_ontime' in ascending order. Identify the bin combinations where the frac_ontime is less than 70%. Based on the two variables, DepDelay and Distance, determine a decision rule for canceling the meeting:\n","### Cancel the meeting if there is more than a 30% probability that the flight will be delayed by more than 15 minutes (not ontime)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxT11WwMcT6z"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhAtD8h5cT6z"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ePg7SFCHcT6z"},"source":["# In the following, you will evaluate the decision rule using the fligths data in year 2024.\n","## Suppose your decision rule is:\n","### - If Distance is between (dist1, dist2) and DepDelay is between (delay1, delay2), you will cancel the meeting (the flight will be delayed). For me, the conditions are: Distance < 2000 and DepDelay > 4.\n","\n","## You evaluate the rule in terms of the following metrics:\n","$$\n","accuracy = \\frac{number\\_of\\_correctly\\_predicted\\_flights}{total\\_number\\_of\\_flights} = \\frac{tp+tn}{tp+tn+fp+fn}\n","$$\n","\n","$$\n","precision = \\frac{number\\_of\\_correctly\\_predicted\\_delayed\\_flights}{total\\_number\\_of\\_predicted\\_delayed\\_flights} = \\frac{tp}{tp+fp}\n","$$\n","\n","$$\n","recall = \\frac{number\\_of\\_correctly\\_predicted\\_delayed\\_flights}{total\\_number\\_of\\_truly\\_delayed\\_flights} = \\frac{tp}{tp+fn}\n","$$"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"51f31726-b1ec-4cf6-81a7-95fa4522890b","showTitle":false,"title":""},"id":"khh8YJWwcT6z"},"source":["## 17. Create a Spark DataFrame `flights_2024` from the 12 CSV files in the 2024_flights subfolder in your GCP Bucket. (If you use Databricks Community Edition which allows up to 10 GB data, ingest the 2024 CSV files to Databricks and create a Spark DataFrame from the ingested data.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ptL_T-JcT6z"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b2558031-3a2d-4cbf-a847-b7d20a21249a","showTitle":false,"title":""},"id":"sIPyELL-cT60"},"source":["## 18. Extract a Spark DataFrame `df_spark_2024` from `flights_2024` with three columns 'Distance',  'DepDelay', and 'ArrDelay' such that the Distance is less than 2000 miles and the DepDelay is between -10 and 40 minutes. Save the total number of flights as 'total_flights'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcpLB6lUcT60"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"e9cZrTnRcT60"},"source":["## 19. From the Spark DataFrame `df_spark_2024`, extract the total deleyed flights (ArrDelay > 15). Save the result as 'total_delayed'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppl5xjZYcT60"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"A8KlusWhcT60"},"source":["## 20. From Spark DataFrame `df_spark_2024`, extract the total deleyed flights if the conditions of the decision rule are met. Save the result as 'true_positives'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3ZnJPdzcT60"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hRKmQG22cT60"},"source":["## 21. From the Spark DataFrame `df_spark_2024`, extract the total on-time flights if the conditions of the decision rule are not met. Save the result as 'true_negatives'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeeAlokKcT60"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"kDMEZC1VcT60"},"source":["## 22. Compute the cells of the confusion matrix. Using the values in the cells to compute the metrics: accuracy, precision, and recall. Discuss the results.\n","\n","![](https://i.imgur.com/oOC7CZ8.png)\n","\n","$$\n","accuracy =  \\frac{tp+tn}{tp+tn+fp+fn}\n","$$\n","\n","$$\n","precision = \\frac{tp}{tp+fp}\n","$$\n","\n","$$\n","recall = \\frac{tp}{tp+fn}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_N3oXq-cT60"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wK49CkqacT60"},"source":["## Discuss the results:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmMP8xMocT60"},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"INFO323-Assignment3","widgets":{}},"environment":{"kernel":"9c39b79e5d2e7072beb4bd59-runtime-00006ae03e73","name":"workbench-notebooks.m129","type":"gcloud","uri":"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"},"kernelspec":{"display_name":"myspark on Serverless Spark (Remote)","language":"python","name":"9c39b79e5d2e7072beb4bd59-runtime-00006ae03e73"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}