{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[{"file_id":"https://github.com/anyuanay/info212/blob/main/INFO212_Week8_Lecture_Data_Loading_Merging.ipynb","timestamp":1731344552116}],"private_outputs":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"-EoniI5UgdWk"},"source":["# INFO 212: Data Science Programming 1\n","___\n","\n","## Week 8: Data Loading and Merging\n","---\n","**Agenda:**\n","- Read and parse tables from HTML files\n"," - pd.read_html(html_source.text)\n","- Read and write JSON data\n"," - json.loads()\n"," - json.dumps()\n"," - pd.read_json()\n","- Combining and merging datasets\n"," - numpy.concatenate(list of narrays, axis)\n"," - pandas.concat(list of DataFrames, axis)\n"," - pd.merge(df1, df2, left_on='key1', right_on='key2', how='outer')\n"," - pd.merge(left1, right1, left_on='key', right_index=True)\n"," - left.join(right, on='key')\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"JejLZBUEK_97"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eY-lBCiTgdWv"},"source":["# import\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dl-XV6AkgdWv"},"source":["# Loading Data\n","## Accessing data is a necessary first step for data analysis. We are going to be focused on data input and output using pandas, though there are numerous tools in other libraries to help with reading and writing data in various formats. Input and output typically falls into a few main categories: reading text files and other more efficient on-disk formats, loading data from databases, and interacting with network sources like web APIs."]},{"cell_type":"markdown","metadata":{"id":"ndQXUq_PgdWw"},"source":["# HTML: Web Scraping\n","\n","### Python has many libraries for reading and writing data in the ubiquitous HTML and XML formats. Examples include lxml, Beautiful Soup, and html5lib. While lxml is comparatively much faster in general, the other libraries can better handle malformed HTML or XML files.\n","\n","### Pandas has a built-in function, read_html, which uses libraries like lxml and Beautiful Soup to automatically parse tables out of HTML files as DataFrame objects. To show how this works, read in an HTML file  from the United States FDIC government agency showing bank failures: (https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/)."]},{"cell_type":"markdown","source":["```\n","import requests\n","\n","target_url = 'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'\n","\n","html_source = requests.get(target_url)\n","\n","html_source.status_code\n","\n","html_source.text\n","\n","tables = pd.read_html(html_source.text)\n","\n","```"],"metadata":{"id":"2jyVJkBVZ25J"}},{"cell_type":"markdown","source":["## Exercise:\n","Run and test the above code."],"metadata":{"id":"ELkbj2YCZ5Lb"}},{"cell_type":"markdown","source":["### Explore the tables."],"metadata":{"id":"TWFv924IaJH2"}},{"cell_type":"code","source":["import requests\n"],"metadata":{"id":"m-PBVhTwaNXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_url = 'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'\n"],"metadata":{"id":"bhMrUTCSMs4J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["html_source = requests.get(target_url)\n","#This is the source"],"metadata":{"id":"h5NIh_JNMt7M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["html_source.status_code\n"],"metadata":{"id":"pQCwz5v7MyXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["html_source.text\n"],"metadata":{"id":"ik-RoEjUM5Q5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tables = pd.read_html(html_source.text)\n","tables[0]"],"metadata":{"id":"I9jgdOfwM78v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(tables)"],"metadata":{"id":"o_nhjqqYOH3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = tables[0]"],"metadata":{"id":"ZddujDzaOKc9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Base URL for the FDIC failed bank list\n","base_url = 'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'\n","\n","# Initialize an empty list to store data from each page\n","all_tables = []\n","\n","# Iterate through pages in increments of 100 rows per page (57 pages)\n","for i in range(57):  # Adjust range if the page count changes\n","    # Generate the URL for each page\n","    url = base_url + str(i)\n","\n","    # Fetch the page\n","    html_source_text = requests.get(url)\n","\n","\n","    tables = pd.read_html(html_source.text)\n","\n","        # Assuming the table is the first on each page, add it to the list\n","    all_tables.append(tables[0])\n","\n","\n","# Concatenate all tables into a single DataFrame vertically\n","df = pd.concat(all_tables)\n","\n","# Show the full table\n","print(df)\n"],"metadata":{"id":"xgZb3OdJPMQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"OBey1CZdP87M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Change the column labels\n","```\n","df.columns = ['Bank', 'City', 'State', 'Cert',\n","       'AI', 'ClosingDate', 'Fund']\n","```"],"metadata":{"id":"gml8FIvO_WDP"}},{"cell_type":"markdown","source":["## Exercise:\n","Run the above code."],"metadata":{"id":"4P0PsC34aTR4"}},{"cell_type":"code","source":["df.columns = ['Bank', 'City', 'State', 'Cert',\n","       'AI', 'ClosingDate', 'Fund']"],"metadata":{"id":"frao4STo-zDn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"si8jKT-gRRuU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"kNopaUaTS_9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"8XPh-EwuRUGA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Parse the datetime column\n","```\n","close_timestamps = pd.to_datetime(df['ClosingDate'])\n","```"],"metadata":{"id":"_XPHEtLY_bbv"}},{"cell_type":"code","source":["df['ClosingDate'] = pd.to_datetime(df['ClosingDate']) #converting the Closing Date column into DateTime format\n","df.head()"],"metadata":{"id":"xVzgk1vAaa1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ClosingDate = df['ClosingDate']\n","ClosingDate.dt.year.value_counts().plot.bar()"],"metadata":{"id":"Ub0iJUZLR6yj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.City.nunique()"],"metadata":{"id":"PZ0JFdsNSymj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['City'].value_counts().head(10).plot.bar()"],"metadata":{"id":"a-ACo5YsS1vK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Most failed bank in 2010 across cities\n","df[df.ClosingDate.dt.year == 2010]['City'].value_counts().head(10).plot.bar()"],"metadata":{"id":"QBHRT2-HTLxW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[df.City.str.contains('Ph')]['City'].value_counts().head(10).plot.bar()"],"metadata":{"id":"3liIvW2EUJsk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Extract some interesting data\n","```\n","close_timestamps.dt.year.value_counts()\n","\n","close_timestamps.dt.year.value_counts().sort_index()\n","```"],"metadata":{"id":"A79IIqz__gMH"}},{"cell_type":"markdown","source":["## Exercise:\n","Run the above code."],"metadata":{"id":"xyUgQwIFap-e"}},{"cell_type":"code","source":[],"metadata":{"id":"CV5BdSsTauAi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yo7pEN7fgdW7"},"source":["# JSON Data\n","## JSON (short for JavaScript Object Notation) has become one of the standard formats for sending data by HTTP request between web browsers and other applications. It is a much more free-form data format than a tabular text form like CSV. Here is an example."]},{"cell_type":"markdown","source":["```\n","\n","\n","obj = \"\"\"\n","{\"name\": \"Wes\",\n"," \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n"," \"pet\": null,\n"," \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},\n","              {\"name\": \"Katie\", \"age\": 38,\n","               \"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}]\n","}\n","```"],"metadata":{"id":"oqKTK9V2b5rb"}},{"cell_type":"code","source":["\n","obj = \"\"\"\n","{\"name\": \"Wes\",\n"," \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n"," \"pet\": null,\n"," \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},\n","              {\"name\": \"Katie\", \"age\": 38,\n","               \"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}]\n","}\n","\"\"\"\n","#we can flatten the list of dictionaries into a DataFrame."],"metadata":{"id":"lIH-FrSDUswK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a DataFrame:\n","```\n","pd.DataFrame([obj])\n","```"],"metadata":{"id":"Iu8sze7QcACv"}},{"cell_type":"code","source":["pd.DataFrame([obj])"],"metadata":{"id":"OrSjMQJ5VAiA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dsmt9T5ogdW8"},"source":["## How to parse a JSON structure into a Python object?\n","```\n","import json\n","\n","json.loads(obj)\n","\n","pd.DataFrame([json.loads(obj)])\n","\n","result = json.loads(obj)\n","\n","result['siblings']\n","\n","```"]},{"cell_type":"code","source":["import json"],"metadata":{"id":"KWE4MzanUhjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jdict = json.loads(obj) #actually loads it in!\n","jdict"],"metadata":{"id":"jyoTUAZ5Uk1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(jdict)"],"metadata":{"id":"9y9j8xeTV3nj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame([json.loads(obj)])"],"metadata":{"id":"ChP6NCD6VTm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jdict['siblings']"],"metadata":{"id":"X7WVyuVTVZSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(jdict['siblings'])"],"metadata":{"id":"V4QRShGgWG8x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise:\n","Run and test the above code."],"metadata":{"id":"QoupgLAUcTqz"}},{"cell_type":"code","source":[],"metadata":{"id":"vTrru5r9cXdZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extract the nested information and create a flatten DataFrame.\n","```\n","sibs = pd.DataFrame(result['siblings'])\n","\n","rows = []\n","for idx, row in sibs.iterrows():\n","    pets = row['pets']\n","    for pet in pets:\n","        cols = {}\n","        cols['name'] = row['name']\n","        cols['age']= row['age']\n","        cols['pet'] = pet\n","        rows.append(cols)\n","\n","pd.DataFrame(rows)\n","```"],"metadata":{"id":"xxvi3bXa4lZY"}},{"cell_type":"code","source":["sibs = pd.DataFrame(result['siblings']) #get the sibling dictionary from jdict"],"metadata":{"id":"sLrnlMi6Wg1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sibs"],"metadata":{"id":"WpdAV-YlXDVk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This is how to iterate a DataFrame\n","rows = []\n","for idx, row in sibs.iterrows(): #This will give you the index and the rows - .iterrows\n","    pets = row['pets']\n","    for pet in pets:\n","        cols = {}\n","        cols['name'] = row['name']\n","        cols['age']= row['age']\n","        cols['pet'] = pet\n","        rows.append(cols)\n","\n","pd.DataFrame(rows) #This gives you a flattened DataFrame.\n","#So a json file is more effiecient in delievering information than a DataFrame"],"metadata":{"id":"HJrZ4ZFtWqCe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sibs_pets = pd.DataFrame(rows)\n","sibs_pets.columns = ['sib_name', 'sib_age', 'pet']\n","sibs_pets"],"metadata":{"id":"_W47IitBY7Y0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sibs_pets['name'] = \"Wes\"\n","sibs_pets"],"metadata":{"id":"-NdRAhl1ZIGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx, row in sibs_pets2.iterrows(): #iterate thrrough each row\n","    for country in jdict['places_lived']:\n","      cols = {}\n","      cols['name'] = row['name']\n","      cols['sib_name'] = row['sib_name']\n","      cols['sib_age']= row['sib_age']\n","      cols['country'] = country\n","      rows.append(cols)\n","\n","pd.DataFrame(rows)"],"metadata":{"id":"RQC2CmpjaUeI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2yGdRpOgdW-"},"source":["## How you convert a JSON object or list of objects to a DataFrame or some other data structure for analysis will be up to you. Conveniently, you can pass a list of dicts (which were previously JSON objects) to the DataFrame constructor and select a subset of the data fields. For example, we did\n","```\n","result_df = pd.DataFrame(result['siblings'])\n","\n","siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])\n","```"]},{"cell_type":"markdown","metadata":{"id":"ks_kD7DKgdW_"},"source":["## The pandas `read_json` can automatically convert JSON datasets in specific arrangements into a Series or DataFrame. For example, let us load\n","```\n","example.json\n","```"]},{"cell_type":"code","source":[],"metadata":{"id":"PabFpQe26RE2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F7jiImxSgdXb"},"source":["# Combining and Merging Datasets"]},{"cell_type":"markdown","metadata":{"id":"KSJO3kVGgdXs"},"source":["## Concatenating Along an Axis\n","## A kind of data combination operation is referred to interchangeably as concatenation, binding, or stacking."]},{"cell_type":"markdown","source":["```\n","df1 = pd.DataFrame([{'name':'Alice', 'Subject':'math', 'Score':95}, {'name':'Bob', 'Subject':'English', 'Score':96}])\n","\n","df2 = pd.DataFrame([['Alice', 'Frank', 'Charlie'], [19, 20, 18], ['alice@drexel', 'frank@drexel', 'charlie@drexel']])\n","\n","df2 = df2.T\n","\n","df2.columns = ['name', 'age', 'email']\n","\n","df3  = pd.concat([df1, df2], axis=1, keys=['df1', 'df2'])\n","\n","pd.concat([df1, df2])\n","\n","```"],"metadata":{"id":"A7ikerwqc9kD"}},{"cell_type":"markdown","source":["## Exercise:\n","Run and test the above code."],"metadata":{"id":"Jhn9fEojc_vg"}},{"cell_type":"code","source":[],"metadata":{"id":"IOQt0GQmdDOB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gc12ZttOgdXt"},"source":["## In the context of pandas objects such as Series and DataFrame, having labeled axes enable you to further generalize array concatenation. In particular, you have a number of additional things to think about:\n","* If the objects are indexed differently on the other axes, should we combine the\n","distinct elements in these axes or use only the shared values (the intersection)?\n","* Do the concatenated chunks of data need to be identifiable in the resulting\n","object?\n","* Does the “concatenation axis” contain data that needs to be preserved? In many\n","cases, the default integer labels in a DataFrame are best discarded during\n","concatenation.\n","\n","## The concat function in pandas provides a consistent way to address each of these concerns. Here is a list of examples to illustrate how it works. Suppose we have three Series with no index overlap:\n","```\n","s1 = pd.Series([0, 1], index=['a', 'b'])\n","s2 = pd.Series([2, 3, 4], index=['c', 'd', 'e'])\n","s3 = pd.Series([5, 6], index=['f', 'g'])\n","```"]},{"cell_type":"markdown","metadata":{"id":"eNgjcci8gdXu"},"source":["## Calling concat with these objects in a list glues together the values and indexes:\n","```\n","pd.concat([s1, s2, s3], axis = 1)\n","```"]},{"cell_type":"code","source":[],"metadata":{"id":"ZbPidvFidbMr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4BLA9Q9VgdXu"},"source":["## By default concat works along axis=0, producing another Series. If you pass axis=1, the result will instead be a DataFrame (axis=1 is the columns):\n","```\n","pd.concat([s1, s2, s3])\n","```"]},{"cell_type":"code","metadata":{"id":"3dz1I07mgdXv"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uazlo_81gdXv"},"source":["## In this case there is no overlap on the other axis, which as you can see is the sorted union (the 'outer' join) of the indexes. You can instead intersect them by passing join='inner':\n","```\n","s4 = pd.concat([s1, s3])\n","pd.concat([s1, s4], axis=1, join='inner')\n","```"]},{"cell_type":"code","metadata":{"id":"57C0tL7dgdXw"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Combine DataFrames:\n","We can concatenate dataframes on axis to combine them vertically or horizontally.\n","```\n","df1 = pd.DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'],\n","                   columns=['one', 'two'])\n","df2 = pd.DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'],\n","                   columns=['three', 'four'])\n","\n","pd.concat([df1, df2], axis=0)\n","\n","pd.concat([df1, df2], axis=1)\n","```"],"metadata":{"id":"7oQTLzmpeJOD"}},{"cell_type":"markdown","source":["## Exercise:\n","Run and test the above code."],"metadata":{"id":"rnYRfkGGeUx9"}},{"cell_type":"code","source":[],"metadata":{"id":"8Nt-8FFpeZCD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XGvJSKmpgdXb"},"source":["## Database-Style DataFrame Joins\n","## Merge or join operations combine datasets by linking rows using one or more keys. These operations are central to relational databases (e.g., SQL-based). The merge function in pandas is the main entry point for using these algorithms on your data. Let’s start with a simple example:"]},{"cell_type":"markdown","source":["```\n","df1 = pd.DataFrame({'key1': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\n","                    'data1': range(7)})\n","df2 = pd.DataFrame({'key2': ['a', 'b', 'd'],\n","                    'data2': range(3)})\n","\n","pd.merge(df1, df2, left_on='key1', right_on='key2', how='outer')\n","\n","df1.merge(df2, left_on='key1', right_on='key2')\n","\n","df1.join(df2)\n","```"],"metadata":{"id":"e0CDofq7esiU"}},{"cell_type":"markdown","source":["## Exercise:\n","Run and test the above code."],"metadata":{"id":"pHgLH3pZevvK"}},{"cell_type":"code","source":[],"metadata":{"id":"1DbJX4aYezfM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-KjosWxNgdXd"},"source":["## Note that I didn't specify which column to join on. If that information is not specified, merge uses the overlapping column names as the keys. It's a good practice to specify explicitly, though:\n","```\n","pd.merge(df1, df2, left_on='key', right_on='key')\n","```"]},{"cell_type":"code","metadata":{"id":"X2qRcWjUgdXe"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2I_wLSPEgdXg"},"source":["## By default merge does an 'inner' join; the keys in the result are the intersection, or the common set found in both tables. Other possible options are 'left', 'right', and 'outer'. The outer join takes the union of the keys, combining the effect of applying both left and right joins:\n","```\n","pd.merge(df1, df2)\n","\n","pd.merge(df1, df2, how='outer')\n","```"]},{"cell_type":"code","metadata":{"id":"cCGf9qzrgdXg"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_VyjMi41gdXj"},"source":["## Merging on Index\n","## In some cases, the merge key(s) in a DataFrame will be found in its index. In this case, you can pass left_index=True or right_index=True (or both) to indicate that the index should be used as the merge key:"]},{"cell_type":"markdown","source":["```\n","left1 = pd.DataFrame({'key': ['a', 'b', 'a', 'a', 'b', 'c'],\n","                      'value': range(6)})\n","right1 = pd.DataFrame({'group_val': [3.5, 7]}, index=['a', 'b'])\n","\n","pd.merge(left1, right1.reset_index(), left_on='key', right_on='index')\n","\n","pd.merge(left1, right1, left_on='key', right_index=True)\n","\n","\n","```"],"metadata":{"id":"rjRf2uOufW0Y"}},{"cell_type":"markdown","source":["## Exercise:\n","Run and test the above code."],"metadata":{"id":"9pIiFI3efeyu"}},{"cell_type":"code","source":[],"metadata":{"id":"K6usionufhZu"},"execution_count":null,"outputs":[]}]}